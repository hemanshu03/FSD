{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  Name -> Deven Chhajed\n",
        "# Roll No-> 32\n",
        "# Batch -> B1 (CSE)\n",
        "# Prn -> 1032210789\n",
        "# Data Transformation and Smoothening"
      ],
      "metadata": {
        "id": "JERokzMECj0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Transformation and Smoothening"
      ],
      "metadata": {
        "id": "NlipO8SbDIgX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Transformation:\n",
        "Data transformation is the process of converting data from one format or structure to another to prepare it for analysis or modeling. This involves cleaning, scaling, encoding, and other operations to make the data suitable for specific tasks. It's a crucial step in data analysis and can significantly impact results."
      ],
      "metadata": {
        "id": "Panys-oiFHdq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importance of the Data Transformation:\n",
        "The importance of data transformation lies in its ability to enhance the quality, suitability, and usability of data for various analytical and modeling tasks.\n",
        "Here are some key reasons why data transformation is crucial:\n",
        "\n",
        "# **Enhancing the Quality of Data:**\n",
        "\n",
        "**Identifying and Managing Outliers:** Data transformation plays a crucial role in recognizing and addressing outliers that, if unaddressed, can negatively impact results and the effectiveness of models.\n",
        "# Improved Model Effectiveness:\n",
        "**Normalization/Standardization:** By scaling numerical features, we ensure that all variables carry equal importance in machine learning algorithms, ultimately resulting in improved model performance.\n",
        "\n",
        "**Encoding Categorical Data:** Transforming categorical variables into numerical representations enhances their compatibility with a broad spectrum of machine learning algorithms, thereby increasing model accuracy.\n",
        "\n",
        "# Enhanced Understandability:\n",
        "**Streamlining:** Data transformation has the ability to streamline intricate datasets, rendering them more comprehensible when utilized in visualizations and reports.\n",
        "\n"
      ],
      "metadata": {
        "id": "uBx6EFbnFeHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the Libraries"
      ],
      "metadata": {
        "id": "5-PTSTmbDH8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**from sklearn.preprocessing import MinMaxScaler:** In Python, the code \"from sklearn.preprocessing import MinMaxScaler\" is used to import the MinMaxScaler class from the scikit-learn (sklearn) library. The MinMaxScaler is a data preprocessing technique used to transform numerical data into a specific range, typically between 0 and 1, by linearly scaling the data. This scaling is commonly used in machine learning to ensure that features with different scales do not dominate the learning process and to make the data more suitable for various algorithms.\n",
        "\n",
        "**import numpy as np:** By including 'import numpy as np' in Python, you gain access to NumPy, a vital numerical computing library. NumPy provides robust support for arrays, matrices, and mathematical functions, making it a cornerstone for scientific and mathematical computations.\n",
        "\n",
        "**import pandas as pd**: Pandas, a Python library for data manipulation and analysis, offers essential data structures such as DataFrames and Series to facilitate efficient data handling and analysis.\n",
        "\n",
        "**from scipy.stats import zscore:**  This function is used for calculating z-scores, which measure how many standard deviations a data point is away from the mean in a dataset. Z-scores are helpful for standardizing and identifying outliers in numerical data, making it easier to compare and analyze data with different scales and distributions.\n",
        "\n",
        "**import matplotlib.pyplot as plt:** matplotlib.pyplot simplifies the creation of diverse plots like lines, bars, scatter plots, and histograms, supporting static, animated, or interactive data visualizations.\n",
        "\n",
        "**import math:** By importing the 'math' module in Python, you can tap into a comprehensive collection of mathematical functions and constants. These include trigonometric functions, logarithms, and fundamental mathematical constants like pi and e, all of which are essential for a wide array of mathematical calculations and operations."
      ],
      "metadata": {
        "id": "cJEhc09PDSbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import zscore\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ],
      "metadata": {
        "id": "ZzS9NAZ1Codw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Transformation Techniques:\n",
        "# **1. Normalization / Standardization:**\n",
        "**Z_Score Normalization:**\n",
        "Z-score normalization, also known as standard score normalization, is a specific form of standardization used to transform a dataset such that it has a mean (average) of 0 and a standard deviation of 1. This technique is particularly useful when you want to compare and analyze data points in terms of their deviation from the mean, regardless of the original units or scales of the data.\n",
        "\n",
        "Here's the formula for z-score normalization:\n",
        "\n",
        "* Z-Score (x_z) = (x - μ) / σ\n",
        "\n",
        "Where:\n",
        "\n",
        "* x is an individual data point in the dataset.\n",
        "\n",
        "* μ (mu) is the mean (average) of the dataset.\n",
        "\n",
        "* σ (sigma) is the standard deviation of the dataset.\n",
        "\n",
        "**Min Max Scaling:**\n",
        "Min-Max scaling, also known as feature scaling or min-max normalization, is a data preprocessing technique used to scale the values of a feature to a specific range, typically between 0 and 1. This ensures that all features have the same scale, making them comparable and preventing features with larger values from dominating the learning process in machine learning algorithms.\n",
        "\n",
        "Here's the formula for Min-Max scaling:\n",
        "\n",
        "* Min-Max Scaled Value (x_scaled) = (x - min(x)) / (max(x) - min(x))\n",
        "\n",
        "Where:\n",
        "\n",
        "* x is an individual data point in the feature.\n",
        "\n",
        "* min(x) is the minimum value in the feature.\n",
        "\n",
        "* max(x) is the maximum value in the feature.\n",
        "\n",
        "**Decimal Scaling:**\n",
        "\n",
        "Decimal scaling, also known as decimal normalization, is a data preprocessing technique used to scale the values of a feature to a specific range between -1 and 1, or any other desired range that is a power of 10. Unlike Min-Max scaling, which scales values between 0 and 1, decimal scaling allows you to choose the range by specifying the number of decimal places.\n",
        "\n",
        "Here's the formula for decimal scaling:\n",
        "\n",
        "* Decimal Scaled Value (x_scaled) = x / 10^n\n",
        "\n",
        "Where:\n",
        "\n",
        "* x is an individual data point in the feature.\n",
        "* n is the number of decimal places you want in the scaling.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2OVC7Fj2IAET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalization"
      ],
      "metadata": {
        "id": "e0vAQHCTa5IN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l=[37, 82, 15, 48, 71, 29, 92, 40, 63, 27, 88, 11, 76, 51, 44, 68, 25, 59]\n",
        "a=np.array(l)\n",
        "b=a.reshape(-1,1)"
      ],
      "metadata": {
        "id": "bYgElH5Aa8Ak"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Min Max Scaling"
      ],
      "metadata": {
        "id": "sxbcQveBbUOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_max_scaled_data=[]\n",
        "for i in a:\n",
        "\tmin_max_scaled_data.append((i-a.min())/(a.max()-a.min()))\n",
        "min_max_scaled_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pM58AscbWd6",
        "outputId": "23a26af9-0d0c-48b7-bb21-779046f2f222"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.32098765432098764,\n",
              " 0.8765432098765432,\n",
              " 0.04938271604938271,\n",
              " 0.4567901234567901,\n",
              " 0.7407407407407407,\n",
              " 0.2222222222222222,\n",
              " 1.0,\n",
              " 0.35802469135802467,\n",
              " 0.6419753086419753,\n",
              " 0.19753086419753085,\n",
              " 0.9506172839506173,\n",
              " 0.0,\n",
              " 0.8024691358024691,\n",
              " 0.49382716049382713,\n",
              " 0.4074074074074074,\n",
              " 0.7037037037037037,\n",
              " 0.1728395061728395,\n",
              " 0.5925925925925926]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Min Max Scaling using Sklean Library"
      ],
      "metadata": {
        "id": "2sj-KSCLbroo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# It is used to import the MinMaxScaler class from scikit-learn (sklearn), which is a tool for scaling numerical features to a specific range, typically between 0 and 1, making them suitable for machine learning algorithms."
      ],
      "metadata": {
        "id": "OEzjDU_dbwLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler=MinMaxScaler()"
      ],
      "metadata": {
        "id": "3tNBaKGMbwgH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_data=scaler.fit_transform(b)\n",
        "print(scaled_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAo6XlSWb0Ro",
        "outputId": "704fb6cc-a1e1-4751-8364-d058e74a27eb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.32098765]\n",
            " [0.87654321]\n",
            " [0.04938272]\n",
            " [0.45679012]\n",
            " [0.74074074]\n",
            " [0.22222222]\n",
            " [1.        ]\n",
            " [0.35802469]\n",
            " [0.64197531]\n",
            " [0.19753086]\n",
            " [0.95061728]\n",
            " [0.        ]\n",
            " [0.80246914]\n",
            " [0.49382716]\n",
            " [0.40740741]\n",
            " [0.7037037 ]\n",
            " [0.17283951]\n",
            " [0.59259259]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Z_Score Calculation"
      ],
      "metadata": {
        "id": "xr9WO3Pgb3DO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z_score=[]\n",
        "for i in a:\n",
        "\tz_score.append((i-a.mean())/a.std())\n",
        "z_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AxD23KXb7Jo",
        "outputId": "30484356-72a5-49e2-abd4-efc7a2794142"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-0.5970216145873387,\n",
              " 1.2629303385501398,\n",
              " -1.5063314583434393,\n",
              " -0.14236669270928842,\n",
              " 0.8082754166720895,\n",
              " -0.9276797395895571,\n",
              " 1.6762529948029128,\n",
              " -0.47302481771150684,\n",
              " 0.4776172916698711,\n",
              " -1.0103442708401118,\n",
              " 1.5109239323018036,\n",
              " -1.6716605208445485,\n",
              " 1.014936744798476,\n",
              " -0.018369895833456513,\n",
              " -0.3076957552103976,\n",
              " 0.6842786197962576,\n",
              " -1.0930088020906663,\n",
              " 0.31228822916876187]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculation of Z_Score Using Scipy Library"
      ],
      "metadata": {
        "id": "MyDNEPIpcLTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z_score_1=zscore(l)\n",
        "z_score_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5LBYYarcQz5",
        "outputId": "c9abfe83-c4fc-47ec-a750-bb902115349f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.59702161,  1.26293034, -1.50633146, -0.14236669,  0.80827542,\n",
              "       -0.92767974,  1.67625299, -0.47302482,  0.47761729, -1.01034427,\n",
              "        1.51092393, -1.67166052,  1.01493674, -0.0183699 , -0.30769576,\n",
              "        0.68427862, -1.0930088 ,  0.31228823])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Smoothening:\n",
        "Data smoothing is a technique used to reduce noise in a dataset by applying mathematical methods. It involves creating a smoother version of the data to reveal underlying patterns or trends while removing random fluctuations. Common methods include moving averages, exponential smoothing, and filters, each with its own way of reducing data noise. Data smoothing is useful in various fields, such as signal processing and time series analysis, to improve data interpretation and analysis.\n",
        "\n",
        "## Data Smoothening Importance:\n",
        "* Noise Reduction: Reduces random fluctuations in data.\n",
        "\n",
        "* Visualization: Enhances data visualization and interpretation.\n",
        "\n",
        "* Analysis: Stabilizes statistical analyses and machine learning.\n",
        "\n",
        "* Forecasting: Improves prediction accuracy in time series data.\n",
        "\n",
        "* Control Systems: Essential for stabilizing control systems.\n",
        "\n",
        "* Signal Processing: Filters out unwanted noise in signals.\n",
        "\n",
        "* Market Analysis: Aids trend identification and decision-making.\n",
        "\n",
        "* Data Quality: Cleans data by removing outliers.\n",
        "\n",
        "* Sensor Data: Ensures accurate readings in IoT and sensors.\n",
        "\n",
        "## Data Smoothening Techniques:\n",
        "\n",
        "**1. Equal Width Binning:** Equal width binning is a data preprocessing technique that involves dividing continuous data into a specified number of equal-width intervals or bins.\n",
        "Binning by mean is a data preprocessing technique that involves grouping data points into bins or intervals based on their proximity to the mean (average) value of the data. This technique is used to create bins where each bin's center corresponds to the mean value of the data within that bin.\n",
        "\n",
        "\n",
        "**2. Binning by Mean:** Binning by mean is a data preprocessing technique that involves grouping data points into bins or intervals based on their proximity to the mean (average) value of the data. This technique is used to create bins where each bin's center corresponds to the mean value of the data within that bin.\n",
        "\n",
        "**3. Equal Frequency Binning:** Equal frequency binning, also known as equi-depth or quantile binning, is a data preprocessing technique used to discretize continuous numerical data into a set of bins or intervals such that each bin contains approximately the same number of data points. This technique is particularly useful when you want to ensure that each bin represents an equal portion of the dataset, making it suitable for handling skewed data distributions.\n",
        "\n",
        "\n",
        "**4. Custom Binning:** Custom binning, also known as manual or user-defined binning, is a data preprocessing technique where you define bins or intervals based on your domain knowledge, specific requirements, or insights about the data. Unlike other binning techniques that use automated rules, custom binning allows you to group data points into bins according to your expertise and understanding of the data.\n",
        "\n",
        "**5. Binning by Boundary:** Binning by boundary is a data preprocessing technique that involves dividing a dataset into bins or intervals based on predefined boundaries or thresholds. Instead of using statistical measures or automated rules, binning by boundary relies on specific values you choose to separate the data into meaningful categories or ranges.\n",
        "\n",
        "**5. Binning by Median:** Binning by median is a data preprocessing technique that involves dividing a dataset into bins or intervals based on the median value of the data. This approach creates bins that balance the data distribution around the median, which is the middle value of a sorted dataset.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ECeyIyqVcqTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d = [0, 4, 12, 16, 16, 18, 24, 26, 28]\n",
        "d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4SH1hiEf5QP",
        "outputId": "d86eda1b-dc55-45c2-d2cf-5ae620b50431"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 4, 12, 16, 16, 18, 24, 26, 28]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d.sort()\n",
        "d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2Mn7QGDgBSi",
        "outputId": "038e2351-307e-430b-c080-57a54463dd99"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 4, 12, 16, 16, 18, 24, 26, 28]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binning\n"
      ],
      "metadata": {
        "id": "BPcpo2D3ggef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binning by Equal Frequency:"
      ],
      "metadata": {
        "id": "Vrpa6hxzgjgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r=max(d)-min(d)\n",
        "print('Range of list is: ',r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaWH667BgoFq",
        "outputId": "9d699e69-1bc7-41df-9081-48fecf873a98"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range of list is:  28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b=r/len(d)\n",
        "bins=math.floor(b)\n",
        "bins"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyMzz8BCgvIR",
        "outputId": "c824676a-f85a-4042-9cfe-1ab12d2fdf8c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bin1=np.zeros((bins,bins))\n",
        "bin1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yj2xAuPtgyv8",
        "outputId": "589b8fa0-a5dc-4cec-d651-b2ed1cf8a2ee"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=0\n",
        "while(a<len(d)):\n",
        "\tfor i in range(0,bins):\n",
        "\t\tfor j in range(0, bins):\n",
        "\t\t\tbin1[i, j] = d[a]\n",
        "\t\t\ta += 1"
      ],
      "metadata": {
        "id": "z4_vSa9Eg3Qb"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bin1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGfGUO9eg8_U",
        "outputId": "a7fa3087-12b1-47f0-8edb-2555cdbaabd5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.,  4., 12.],\n",
              "       [16., 16., 18.],\n",
              "       [24., 26., 28.]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binning by Mean:"
      ],
      "metadata": {
        "id": "lQKMtaCvhAcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bin2=np.zeros((bins,bins))\n",
        "bin2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-97fePnhFwg",
        "outputId": "838a0da2-767c-4047-c197-8201d94b19ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,len(d),bins):\n",
        "\tmean=(d[i]+d[i+1]+d[i+2])/bins\n",
        "\tk=int(i/bins)\n",
        "\tfor j in range(0,bins):\n",
        "\t\tbin2[k,j]=mean\n",
        "bin2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqYN54llhIR5",
        "outputId": "55a6fcd9-4ecf-4f22-dc69-94cb83a1eda1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 5.33333333,  5.33333333,  5.33333333],\n",
              "       [16.66666667, 16.66666667, 16.66666667],\n",
              "       [26.        , 26.        , 26.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binning by Bin Boundary"
      ],
      "metadata": {
        "id": "5afzOFAGkQlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bin3=np.zeros((bins,bins))\n",
        "bin3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Pd-laK6kQtv",
        "outputId": "cf8b90cb-e85e-4c9d-ae56-a855ddb6c24e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (0,len(d),bins):\n",
        "    k=int(i/bins)\n",
        "    for j in range (bins):\n",
        "        if (d[i+j]-d[i]) < (d[i+2]-d[i+j]):\n",
        "            bin3[k,j]=d[i]\n",
        "        else:\n",
        "            bin3[k,j]=d[i+2]\n",
        "bin3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fpi32gAkaPN",
        "outputId": "241337ca-88ec-4b74-bc60-ac2a2983ce34"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.,  0., 12.],\n",
              "       [16., 16., 18.],\n",
              "       [24., 28., 28.]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}